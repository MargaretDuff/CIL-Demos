{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#  Copyright 2025 United Kingdom Research and Innovation\n",
    "#  Copyright 2025 The University of Manchester\n",
    "#  Copyright 2025 Technical University of Denmark\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "#\n",
    "#   Authored by:    Hannah Robarts (UKRI-STFC)\n",
    "#                   Laura Murgatroyd (UKRI-STFC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIL-ESRF pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains example scripts for loading, pre-processing, reconstructing and visualising tomography data collected at ESRF beamlines. The steps are designed to be adaptable for different kinds of synchrotron datasets and covers commonly used methods, including:\n",
    "- Extracting experiment information and configuring a CIL `AcquisitionData` object\n",
    "- Applying CIL pre-processors, including `Normaliser`, `TransmissionAbsorptionConverter`, `CentreOfRotationCorrector` and `PaganinProcessor`\n",
    "- Reconstructing using filtered back projection with CIL's wrapper for tigre\n",
    "\n",
    "This example uses dataset tomo_00065 from the TomoBank [[1](https://iopscience.iop.org/article/10.1088/1361-6501/aa9c19)] multidistance dataset. The sample is a steel sphere measured at various propagation distances to demonstrate the effect of propagation based phase contrast imaging.\n",
    "\n",
    "The tomo_00065.h5 dataset can be retrieved from https://tomobank.readthedocs.io/en/latest/source/data/docs.data.phasecontrast.html#multi-distance using:\n",
    "\n",
    "`wget https://g-a0400.fd635.8443.data.globus.org/tomo_00064_to_00067/tomo_00065.h5`\n",
    "\n",
    "[1] De Carlo, Francesco, et al. “TomoBank: a tomographic data repository for computational x-ray science.” Measurement Science and Technology 29.3 (2018): 034004. http://www.doi.org/10.1088/1361-6501/aa9c19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# CIL methods\n",
    "from cil.framework import DataContainer\n",
    "from cil.utilities.display import show2D, show_geometry\n",
    "from cil.utilities.jupyter import islicer\n",
    "from cil.io.utilities import HDF5_utilities\n",
    "from cil.io import TIFFWriter\n",
    "from cil.processors import Normaliser, RingRemover, TransmissionAbsorptionConverter, CentreOfRotationCorrector, PaganinProcessor\n",
    "from cil.recon import FBP\n",
    "# Additional packages\n",
    "import numpy as np # conda install numpy\n",
    "import matplotlib.pyplot as plt # conda install matplotlib\n",
    "# Custom methods\n",
    "from esrf_code.HDF5_ParallelDataReader import HDF5_ParallelDataReader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the file and use `HDF5_utilities` to print the metadata and find the locations of data and scan information within the file. We see there is a lot of information about the experiment we can use to help with the processing and reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/mnt/materials/SIRF/Fully3D/CIL/Phase/tomo_00065.h5'\n",
    "HDF5_utilities.print_metadata(filename) # comment out if you don't want to see the metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure paths to the relevant data and metadata in the file, then read the data using the generic `HDF5_ParallelDataReader`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = HDF5_ParallelDataReader(filename, \n",
    "                                 dataset_path=('exchange/data'),\n",
    "                                 distance_units='mm', angle_units='degree')\n",
    "\n",
    "reader.configure_angles(angles_path='exchange/theta', HDF5_units='degree')\n",
    "\n",
    "reader.configure_pixel_sizes('measurement/instrument/detector/x_actual_pixel_size',\n",
    "                             'measurement/instrument/detector/y_actual_pixel_size',\n",
    "                             HDF5_units = 'um')\n",
    "\n",
    "reader.configure_normalisation_data(flatfield_path='exchange/data_white',\n",
    "                                    darkfield_path='exchange/data_dark')\n",
    "\n",
    "reader.configure_sample_detector_distance(sample_detector_distance=58, HDF5_units='mm') # required for phase retrieval\n",
    "\n",
    "data = reader.read()\n",
    "\n",
    "energy = HDF5_utilities.read(filename, 'measurement/instrument/monochromator/energy') # required for phase retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `islicer` to visualise the data. Try looking through the projections by sliding the slice index slider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "islicer(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `show_geometry()` to check the orientation of the sample and detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_geometry(data.geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Normalise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uneven background suggests the data needs to be normalised. We use the flat and dark scans that we loaded as part of the data reader and the CIL `Normaliser` method. Here we have multiple flat and dark scans so we take the mean along the first axis of each. To learn more about the parameters for `Normaliser`, check CIL's documentation https://tomographicimaging.github.io/CIL/v24.3.0/processors/#data-normaliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Normaliser(flat_field=np.mean(reader.flatfield.array, axis = 0), dark_field=np.mean(reader.darkfield.array, axis = 0))\n",
    "processor.set_input(data)\n",
    "data_test = processor.get_output()\n",
    "\n",
    "# Use the show2D method to check the effect of the normalisation\n",
    "show2D([data, data_test],\n",
    "       title=['Before Normalisation', 'After Normalisation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Look at the sinogram for a detector row, here we select vertical index=460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_slice = 460\n",
    "show2D([data, data_test], slice_list=('vertical', vertical_slice),\n",
    "       title=['Before Normalisation', 'After Normalisation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're happy with the outcome of the processor, copy the results from data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Transmission to absorption "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the CIL `TransmissionAbsorptionConverter` which applies the Beer-Lambert law, to view the data in the absorption domain. If there are negative numbers in the data, specify a low value in `min_intensity` to clip these values before calculating $-log()$, check CIL's documentation for more details about configuring this processor https://tomographicimaging.github.io/CIL/v24.3.0/processors/#transmission-to-absorption-converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = TransmissionAbsorptionConverter()(data)\n",
    "show2D([data, data_test], ['Before transmission to absorption correction','After transmission to absorption correction'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_test.copy() # copy the data if we're happy with the processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Filtered back projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the CIL Filtered Back Projection `FBP` method to check the reconstruction on a single vertical slice of the data. The FBP method in the recon class uses `tigre` by default but can alternatively be configured for use with the  `backend = astra`. These use projectors from the tigre and astra packages respectively, see CIL's documentation for more details https://tomographicimaging.github.io/CIL/v24.3.0/recon/#fbp-reconstructor-for-parallel-beam-geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_slice = data.get_slice(vertical=vertical_slice)\n",
    "reco = FBP(data_slice).run(verbose=False)\n",
    "show2D(reco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Centre of rotation correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various artefacts can be observed in the reconstruction if the sample is not perfectly at the centre of the rotation stage. This dataset is from a parallel beam experiment and it has projections from 360 degrees around the sample, which results in a doubling effect if the centre of rotation is offset. We can remove the artefacts by accounting for the offset in the reconstruction. \n",
    "\n",
    "We can find the correct offset by looping through different pixel values manually and view the reconstructions using `islicer` to choose the offset where rotation artefacts are minimised. Run the cell below and vary the slice index to see the effect of using different pixel offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_list = []\n",
    "pixel_offsets = [-10, -15, -20, -25]\n",
    "for p in pixel_offsets:\n",
    "    data_test.geometry.set_centre_of_rotation(p, distance_units='pixels')\n",
    "    data_slice = data_test.get_slice(vertical=vertical_slice)\n",
    "    reco_test = FBP(data_slice).run(verbose=False)\n",
    "    array_list.append(reco_test.array)\n",
    "DC = DataContainer(np.stack(array_list, axis=0), dimension_labels=tuple(['Centre of rotation offset']) + reco_test.geometry.dimension_labels)\n",
    "islicer(DC, title=tuple(['Centre of rotation offset: ' + str(p)  + ', index: ' for p in pixel_offsets]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, there are two methods in CIL which can automatically identify the correct centre of rotation\n",
    "- The CIL `CentreOfRotationCorrector.xcorrelation` processor finds the centre of rotation offset automatically by comparing a single slice of projections 180 degrees apart and minimising the difference between them. It can be used on parallel beam data.\n",
    "- Here we use the `CentreOfRotationCorrector.image_sharpness` processor which maximises the sharpness of a reconstructed slice. It can be used on single slice parallel-beam, and centre-slice of cone-beam geometry with 360 degrees of data.\n",
    "Check CIL's documentation for details of the methods for correcting the centre of rotation https://tomographicimaging.github.io/CIL/v24.3.0/processors/#centre-of-rotation-corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = CentreOfRotationCorrector.image_sharpness()(data)\n",
    "\n",
    "# Check the effect on the reconstruction\n",
    "data_slice = data_test.get_slice(vertical=vertical_slice)\n",
    "reco_test = FBP(data_slice).run(verbose=False)\n",
    "show2D([reco, reco_test],\n",
    "['Before centre of rotation correction','After centre of rotation correction'])\n",
    "# And a zoomed in plot of the slice\n",
    "show2D([reco.array[200:300,150:250], reco_test.array[200:300, 150:250]],\n",
    "       title=['Before centre of rotation correction','After centre of rotation correction'],\n",
    "       axis_labels=('horizontal_x', 'horizontal_y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Print the geometry to see the rotation axis has been changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Centre of rotation before {}\"\n",
    "      .format(data.geometry.get_centre_of_rotation(distance_units='pixels')))\n",
    "print(\"Centre of rotation after {}\"\n",
    "      .format(data_test.geometry.get_centre_of_rotation(distance_units='pixels')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_test.copy() # copy the data if we're happy with the processor\n",
    "reco = reco_test.copy() # copy the reconstruction if we're happy with the processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ring removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ring artefacts appear in the reconstruction where dead or varying pixels remain in the projections. Various methods exist to remove these from the reconstruction. Here we use the CIL `RingRemover` which removes stripes in the sinogram via a wavelet decomposition method. Try varying different parameters on a vertical slice of the dataset and see the effect on the rings in the reconstruction (for slice 460 there is a small ring at the centre of the reconstruction)\n",
    "- The `decNum` parameter defines the number of wavelet decompositions used. Increasing decNum will increase the ring remover strength, but increases the computational effort and may distort the shape of the data.\n",
    "- `wname` defines the filter name to use from `'db1' -- 'db35', 'haar'` - increasing the wavelet filter number increases the strength of the ring removal, but also increases the computational effort\n",
    "- `sigma` describes the damping parameter in Fourier space - increasing sigma, increases the size of artefacts which can be removed\n",
    "\n",
    "Find more details about the ring remover method here https://tomographicimaging.github.io/CIL/v24.3.0/processors/#ring-remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_list = []\n",
    "array_list.append(reco.array) # include the original reconstruction for comparison\n",
    "sigma = 0.01\n",
    "wname = \"db5\"\n",
    "decNum_list = [1, 2, 3, 4]\n",
    "for d in decNum_list:\n",
    "    data_slice = data_test.get_slice(vertical=vertical_slice)\n",
    "    data_slice = RingRemover(decNum = d, wname = wname, sigma = sigma,  info = False)(data_slice)\n",
    "    reco_test = FBP(data_slice).run(verbose=False)\n",
    "    array_list.append(reco_test.array)\n",
    "DC = DataContainer(np.stack(array_list, axis=0), dimension_labels=tuple(['Ring remover decNum']) + reco.geometry.dimension_labels)\n",
    "islicer(DC, title=tuple(['No ring remover'] + ['Ring remover decNum: ' + str(p) + ', index: ' for p in decNum_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that using small parameters (e.g. `sigma=0.01, wname=\"db5\"` and `decNum=1`) gives the most effective the ring removal without introducing new artefacts, so we apply this method to the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.01\n",
    "wname = \"db5\"\n",
    "decNum = 1\n",
    "data_test = RingRemover(decNum = decNum, wname = wname, sigma = sigma,  info = False)(data)\n",
    "\n",
    "# Compare a slice of the reconstruction\n",
    "data_slice = data_test.get_slice(vertical=vertical_slice)\n",
    "reco_test = FBP(data_slice).run(verbose=False)\n",
    "show2D([reco, reco_test],\n",
    "       title=[\"Before ring removal\", \"After ring removal\"])\n",
    "# And a zoomed in plot of the slice\n",
    "show2D([reco.array[100:250, 100:250], reco_test.array[100:250,100:250]],\n",
    "       title=[\"Before ring removal\", \"After ring removal\"],\n",
    "       axis_labels=('horizontal_x', 'horizontal_y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_test.copy() # copy the data if we're happy with the processor\n",
    "reco = reco_test.copy() # copy the reconstruction if we're happy with the processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Phase retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bright edges in the reconstruction are an example of edge enhancement due to phase contrast. In this experiment, propagation-based phase contrast imaging was used to exploit the different contrast provided by absorption and phase. Phase retrieval methods can be used to separate out the phase and intensity information. CIL implements the common Paganin phase retrieval method (see [https://doi.org/10.1046/j.1365-2818.2002.01010.x](https://onlinelibrary.wiley.com/doi/10.1046/j.1365-2818.2002.01010.x)) which results in a boost to the signal to noise ratio (SNR) without losing spatial resolution and so is a commonly used pre-processing step, however, it can result in blurring out useful features so should be used with care!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Run the CIL `PaganinProcessor`\n",
    "- `delta` and `beta` are the real and complex part of the material refractive index. Increasing the ratio of `delta/beta` increases the strength of the filter, here we've chosen the parameters to remove fringes. Try varying the strength to see the effect on the reconstruction.\n",
    "- `full_retrieval = False` means the calculation does not include $-log()$. If we apply the phase retrieval before converting to absorption we should use `full_retrieval = True`\n",
    "\n",
    "For more information about using the `PaganinProcessor` in CIL, check the documentation https://tomographicimaging.github.io/CIL/v24.3.0/processors/#paganin-processor or for a more detailed explanation of the effect of different parameters, see the phase retrieval demo in the deep dive folder [demos/4_Deep_Dives/02_phase_retrieval.ipynb](https://github.com/TomographicImaging/CIL-Demos/blob/main/demos/4_Deep_Dives/02_phase_retrieval.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 3e-5\n",
    "beta = 2e-10\n",
    "processor = PaganinProcessor(delta=delta, beta=beta, \n",
    "                             energy=energy, energy_units='keV', \n",
    "                             full_retrieval=False)\n",
    "processor.set_input(data)\n",
    "data_test = processor.get_output()\n",
    "\n",
    "# Compare a zoomed-in slice of the reconstruction\n",
    "data_slice = data_test.get_slice(vertical=vertical_slice)\n",
    "reco_test = FBP(data_slice).run(verbose=False)\n",
    "\n",
    "show2D([reco, reco_test],\n",
    "       [\"Before phase retrieval\", \"After phase retrieval\"])\n",
    "show2D([reco.array[200:300, 200:300], reco_test.array[200:300, 200:300]],\n",
    "       title=[\"Before phase retrieval\", \"After phase retrieval\"],\n",
    "       fix_range=(0.00, 0.04),\n",
    "       axis_labels=('horizontal_x', 'horizontal_y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the bright contrast at the edge is reduced, sample features are blurred and noise is reduced. Plot a cross-section through the edge of the sample to look more closely at the fringes caused by the phase contrast. We can see a sharp peak/ fringe at the sample edge in the reconstruction before the phase retrieval and after phase retrieval the fringe is removed. The sample feature (at horizontal_x, horizontal_y=35 in the plot above) is reduced in intensity and blurred but we find the SNR of the feature is improved. \n",
    "\n",
    "It's important to note that phase retrieval might be useful if the improved SNR is important, or to maintain the precise shape of sample features - for example in metrology. However, it should be used with caution if you want to exploit the advantages of phase contrast - for example using edge enhancement to aid segmentation. You should consider carefully whether the phase retrieval step should be used on each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reco.array[235,200:300])\n",
    "plt.plot(reco_test.array[235,200:300])\n",
    "plt.grid()\n",
    "plt.xlabel('Horizontal x (pixels)')\n",
    "plt.ylabel('Intensity')\n",
    "plt.legend(['Before phase retrieval','After phase retrieval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original reconstruction SNR = {:.3f}\".format(np.abs(reco.mean()/reco.array.std())))\n",
    "print(\"Phase retrieved reconstruction SNR = {:.3f}\".format(np.abs(reco_test.mean()/reco_test.array.std())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = reco.array[235,210:260]\n",
    "print(\"Sample feature SNR = {:.3f}\".format(np.abs(y.mean()/y.std())))\n",
    "y = reco_test.array[235,210:260]\n",
    "print(\"Sample feature SNR after phase retrieval = {:.3f}\".format(np.abs(y.mean()/y.std())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_test.copy() # copy the data if we're happy with the processor\n",
    "reco = reco_test.copy() # copy the reconstruction if we're happy with the processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### The final reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct the whole dataset then view the reconstruction in `islicer`. \n",
    "\n",
    "Try exploring the data and adjusting the display settings.\n",
    "\n",
    "Notice how some of the processor parameters we configured for the single slice might need to be edited when applied to the full dataset, such as the ring remover strength. Similarly, the phase retrieval step works best for vertical slices with neighbouring slices that contain similar materials, therefore we notice artefacts in the first and last vertical slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "reco = FBP(data).run(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "islicer(reco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Save the processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we're happy with the reconstruction save the processed data as TIFF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = TIFFWriter()\n",
    "writer.set_up(data = data, file_name='path_to_data/data.tiff')\n",
    "# writer.write() # uncomment to save the reconstruction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil_demos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
